{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metodo_de_la_potencia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8sqdBs6TDE",
        "colab_type": "text"
      },
      "source": [
        "# Método de la Potencia\n",
        "\n",
        "En muchas aplicaciones del mundo real de la ciencia y la ingeniería, se requiere encontrar numéricamente el valor Eigen más grande o dominante y el Eigenvector correspondiente. Existen diferentes como el método de la potencia que sigue un enfoque iterativo y conforma un algoritmo simple.\n",
        "\n",
        "El siguiente algoritmo busca desarrollar los pasos necesario para el método de la potencia y así después pues ser implementado como otro método alternativo a los demás que se tienen en el proyecto:\n",
        "\n",
        "Sea **A** una matriz cuadrada de orden **n**, es decir, **An x n**. El Método de Potencia comienza con una aproximación inicial al Eigenvector correspondiente al valor Eigen más grande de tamaño **n x 1**. Sea esta aproximación inicial **Xn x 1**.\n",
        "\n",
        "Después de la suposición inicial, calculamos **AX**, es decir, el producto de la matriz **A** y **X**. Del producto de **AX** dividimos cada elemento por el elemento más grande (por magnitud) y los expresamos como **λ1X1**. El valor obtenido de **λ1** y **X1** es el siguiente valor aproximado mejor del valor Eigen más grande y el Eigenvector correspondiente.\n",
        "\n",
        "Del mismo modo, para el siguiente paso, multiplicamos **A** por **X1**. Del producto de **AX1** dividimos cada elemento por el elemento más grande (por magnitud) y los expresamos como **λ2X2**. El valor obtenido de **λ2** y **X2** es el siguiente valor aproximado mejor del valor Eigen más grande y el Eigenvector correspondiente.\n",
        "\n",
        "Se repite lo anterior iterativamente hasta obtener el valor Eigen más grande o dominante y el Eigenvector correspondiente con la precisión deseada.\n",
        "\n",
        "El algoritmo sería programado de la siguiente forma:\n",
        "\n",
        " \n",
        "  1. Se lee el orden de la matriz (n) y el error tolerable (e)\n",
        " \n",
        "  2. Se lee la matriz A de tamaño n x n\n",
        " \n",
        "  3. Se crea un vector inicial X de tamaño n x 1\n",
        " \n",
        "  4. Inicializar: Lambda_Old = 1\n",
        " \n",
        "  5. Multiplicar: X_NEW = A * X\n",
        " \n",
        "  6. Reemplace X por X_NEW\n",
        " \n",
        "  7. Encuentra el elemento más grande (Lamda_New) por magnitud desde X_NEW\n",
        " \n",
        "  8. Normalizar o dividir X entre Lamda_Nuevo\n",
        " \n",
        "  9. Mostrar Lamda_New y X\n",
        "\n",
        "  10. Si | Lambda_Old - Lamda_New | > e entonces\n",
        "      establecer Lambda_Old = Lamda_New y volver a ejecutar desde el paso 6, de lo contrario detener el algoritmo.\n",
        "\n",
        "\n",
        "Basado en las notas y ejemplo de:\n",
        "\n",
        "Power Method Algorithm for Finding Dominant Eigen Value and Eigen Vector. (n.d.). Retrieved May 23, 2020, from https://www.codesansar.com/numerical-methods/power-method-algorithm-for-finding-dominant-eigen-value-and-eigen-vector.htm\n",
        "\n",
        "Función de R para generar ejemplos:\n",
        "\n",
        "Fox, J., Chalmers, P., Monette, G., & Sanchez, G. (2020, April 14). powerMethod: Power Method for Eigenvectors in matlib: Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics. Retrieved from https://rdrr.io/cran/matlib/man/powerMethod.html\n",
        "\n",
        "Función que integra PCA:\n",
        "\n",
        "Dan, D. J. (n.d.). dianejdan/Power-Method-PCA. Retrieved May 27, 2020, from https://github.com/dianejdan/Power-Method-PCA/blob/master/power-pca.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy8DD4cgT2b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "2d43eec0-f64d-4596-e147-a1894d524b5a"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "#Se genera una matriz como el ejemplo de las notas\n",
        "np.random.seed(2020)\n",
        "mpoints=200\n",
        "ncols = 3\n",
        "X = (np.random.rand(ncols,ncols)@np.random.normal(0,1,(ncols,mpoints))).T\n",
        "m,n = X.shape\n",
        "\n",
        "### Se centran los datos y se calcula la matriz de covarianzas\n",
        "X_centered = X-np.mean(X, axis=0)\n",
        "cov = np.dot(X_centered.T, X_centered)/X_centered.shape[0]\n",
        "\n",
        "print('Matriz de Covarianzas')\n",
        "print(cov)\n",
        "\n",
        "### Método de la potencia para PCA\n",
        "eps = 1e-06 #parámetro de convergencia para finalizar las iteraciones, se deja en 1e-06 conforme a la documentación de R\n",
        "dif=1\n",
        "\n",
        "#Inicializar: Lambda_Old = 1\n",
        "#Multiplicar: X_NEW = A * X\n",
        "#Reemplace X por X_NEW\n",
        "#Encuentra el elemento más grande (Lamda_New) por magnitud desde X_NEW\n",
        "#Normalizar o dividir X entre Lamda_Nuevo\n",
        "#Mostrar Lamda_New y X\n",
        "#Si | Lambda_Old - Lamda_New | > e entonces establecer Lambda_Old = Lamda_New y volver a ejecutar desde el paso 6, de lo contrario detener el algoritmo.\n",
        "\n",
        "#Se crean dos matrices con 1's\n",
        "w = np.ones(X.shape[1])\n",
        "w_new = np.ones(X.shape[1])\n",
        "\n",
        "\n",
        "while (dif > eps): #Iteramos hasta llegar al parámetro de convergencia\n",
        "    w = w_new\n",
        "    w_tmp = np.dot(cov, w)  #se multiplica la matriz de covarianzas por w.\n",
        "    ind = np.abs(w_tmp).argmax() #Calculate the absolute value element-wise. #Returns the indices of the maximum values along an axis.\n",
        "    w_new = w_tmp/w_tmp[ind]\n",
        "    dif = np.dot(w_new-w, w_new-w)\n",
        "\n",
        "eig_val = np.dot(np.dot(cov, w_new), w_new)/np.dot(w_new, w_new)\n",
        "eig_vec = w_new/np.sqrt(np.dot(w_new, w_new))\n",
        "\n",
        "eig_val_true, eig_vec_true = np.linalg.eig(cov)\n",
        "\n",
        "print('El valor Eigen más grande es' % eig_val)\n",
        "print('Su eigenvector correspondiente es')\n",
        "print(eig_vec)\n",
        "\n",
        "\n",
        "### Calcula la proporción de la varianza explicada por los dos primeros PCs\n",
        "X_proj = np.dot(X_centered, eig_vec_true[:,[0,1]])\n",
        "var1 = np.var(X_proj[:,0])\n",
        "var2 = np.var(X_proj[:,1])\n",
        "print('Suma de la varianza de los eigenvectores' % (var1+var2))\n",
        "\n",
        "###  proyecta los datos que apuntan al espacio abarcado por las PC que explican más del 90% de la varianza\n",
        "def calPCA(sigma, comp):\n",
        "    eig_vals, eig_vecs = np.linalg.eig(sigma)\n",
        "    return {'eig_val': eig_vals[comp-1], 'eig_vec': eig_vecs[:,comp-1]}\n",
        "\n",
        "\n",
        "total_var = 0\n",
        "for i in range(X.shape[1]):\n",
        "    total_var = total_var + calPCA(cov, i+1)['eig_val']\n",
        "    \n",
        "prop = 0\n",
        "comp = 1 \n",
        "cur_var = 0\n",
        "comp_vecs = np.zeros([X.shape[1], X.shape[1]])   \n",
        "while (prop < 0.9):\n",
        "    cur_var = cur_var + calPCA(cov, comp)['eig_val']\n",
        "    prop = cur_var/total_var\n",
        "    comp_vecs[:,comp-1] = calPCA(cov, comp)['eig_vec']\n",
        "    comp = comp+1\n",
        "\n",
        "X_new = np.dot(X, comp_vecs[:,range(comp-1)]) \n",
        "print('Las nuevas coordenadas de los primeros 10 puntos de datos son')\n",
        "print(X_new[0:10,])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz de Covarianzas\n",
            "[[1.87079983 0.6268753  0.84175899]\n",
            " [0.6268753  0.21691674 0.31579903]\n",
            " [0.84175899 0.31579903 0.77059722]]\n",
            "The largest eigenvalue I calculated is 2.54\n",
            "\n",
            "Its correspondin eigenvector is\n",
            "[0.84313138 0.28933077 0.45322973]\n",
            "\n",
            "\n",
            "The largest eigenvalue calculated by np.linalg.eig is 2.54\n",
            "\n",
            "Its correponding eigenvector is\n",
            "[-0.84315718 -0.28933414 -0.45317956]\n",
            "\n",
            "\n",
            "sum of variance of projection on the first two eigenvectors is 2.85\n",
            "\n",
            "The new coordinates of the first 10 data points are\n",
            "[[-0.40474378  0.17337183]\n",
            " [ 0.80744656 -0.03100271]\n",
            " [-1.52757559  0.02667735]\n",
            " [ 0.34605851 -0.58106658]\n",
            " [ 1.70443306  0.26647624]\n",
            " [ 2.81431215 -0.03352812]\n",
            " [ 0.87392525 -0.15708527]\n",
            " [-0.4238356  -0.24135029]\n",
            " [ 2.21278866  0.37628692]\n",
            " [ 0.10525245 -0.16856843]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geAibHqRDY7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from pylab import *\n",
        "\n",
        "#Se genera una matriz como el ejemplo de las notas\n",
        "np.random.seed(2020)\n",
        "mpoints=200\n",
        "ncols = 3\n",
        "X = (np.random.rand(ncols,ncols)@np.random.normal(0,1,(ncols,mpoints))).T\n",
        "m,n = X.shape\n",
        "\n",
        "def power_iteration(A, num_simulations: int):\n",
        "    # Ideally choose a random vector\n",
        "    # To decrease the chance that our vector\n",
        "    # Is orthogonal to the eigenvector\n",
        "    b_k = np.random.rand(A.shape[1])\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        # calculate the matrix-by-vector product Ab\n",
        "        b_k1 = np.dot(A, b_k)\n",
        "\n",
        "        # calculate the norm\n",
        "        b_k1_norm = np.linalg.norm(b_k1)\n",
        "\n",
        "        # re normalize the vector\n",
        "        b_k = b_k1 / b_k1_norm\n",
        "\n",
        "    return b_k\n",
        "    \n",
        "pw= power_iteration(X, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
